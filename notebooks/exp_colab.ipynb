{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 KV Cache Experiments (gpt2_optim)\n",
        "\n",
        "This notebook builds and runs the KV cache experiments under `gpt2_optim/`.\n",
        "It focuses on: correctness validation, speed comparison, and profiling.\n",
        "\n",
        "Assumptions:\n",
        "- CUDA is available (Colab GPU runtime).\n",
        "- You have access to `gpt2_124M.bin` and `gpt2_tokenizer.bin` (downloaded via `llm.c` starter pack).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Cloning the repository and building the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llm.c'...\n",
            "remote: Enumerating objects: 6149, done.\u001b[K\n",
            "remote: Total 6149 (delta 0), reused 0 (delta 0), pack-reused 6149 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6149/6149), 2.25 MiB | 6.01 MiB/s, done.\n",
            "Resolving deltas: 100% (3971/3971), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf llm.c\n",
        "!git clone https://github.com/karpathy/llm.c.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded tiny_shakespeare_val.bin to /content/llm.c/dev/data/tinyshakespeare/tiny_shakespeare_val.bin\n",
            "Downloaded tiny_shakespeare_train.bin to /content/llm.c/dev/data/tinyshakespeare/tiny_shakespeare_train.bin\n",
            "Downloaded gpt2_tokenizer.bin to /content/llm.c/dev/../gpt2_tokenizer.bin\n",
            "Downloaded gpt2_124M_bf16.bin to /content/llm.c/dev/../gpt2_124M_bf16.bin\n",
            "Downloaded gpt2_124M.bin to /content/llm.c/dev/../gpt2_124M.bin\n",
            "Downloaded gpt2_124M_debug_state.bin to /content/llm.c/dev/../gpt2_124M_debug_state.bin\n",
            "Downloaded hellaswag_val.bin to /content/llm.c/dev/data/hellaswag/hellaswag_val.bin\n",
            "All files downloaded and saved in their respective directories\n"
          ]
        }
      ],
      "source": [
        "!cd llm.c && chmod u+x dev/download_starter_pack.sh && ./dev/download_starter_pack.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt2_optim'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 64 (delta 36), reused 50 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (64/64), 36.11 KiB | 18.05 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf gpt2_optim\n",
        "!git clone https://github.com/agridrama/gpt2_optim.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/cuda/bin/nvcc --threads=0 -t=0 --use_fast_math -std=c++17 -O3 --generate-code arch=compute_75,code=[compute_75,sm_75] -DENABLE_BF16 -I/content/gpt2_optim/src -I../llm.c /content/gpt2_optim/src/inference_gpt2_optimize.cu -lcublas -lcublasLt -lnvidia-ml -lnvToolsExt  -o /content/gpt2_optim/bin/inference_gpt2optimcu\n",
            "/content/gpt2_optim/src/inference_gpt2_optimize.cu(72): warning #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "      multi_gpu_config = multi_gpu_config_init(1, 0, 1, empty_str, empty_str, \"mpi\");\n",
            "                                                                              ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "/content/gpt2_optim/src/inference_gpt2_optimize.cu(72): warning #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "      multi_gpu_config = multi_gpu_config_init(1, 0, 1, empty_str, empty_str, \"mpi\");\n",
            "                                                                              ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "/usr/local/cuda/bin/nvcc --threads=0 -t=0 --use_fast_math -std=c++17 -O3 --generate-code arch=compute_75,code=[compute_75,sm_75] -DENABLE_BF16 -I/content/gpt2_optim/src -I../llm.c /content/gpt2_optim/src/validate_kvcache_optimization.cu -lcublas -lcublasLt -lnvidia-ml -lnvToolsExt  -o /content/gpt2_optim/bin/validate_kvcache_optimization\n",
            "/content/gpt2_optim/src/validate_kvcache_optimization.cu(35): warning #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "      multi_gpu_config = multi_gpu_config_init(1, 0, 1, empty_str, empty_str, \"mpi\");\n",
            "                                                                              ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "/content/gpt2_optim/src/validate_kvcache_optimization.cu(35): warning #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "      multi_gpu_config = multi_gpu_config_init(1, 0, 1, empty_str, empty_str, \"mpi\");\n",
            "                                                                              ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "/usr/local/cuda/bin/nvcc --threads=0 -t=0 --use_fast_math -std=c++17 -O3 --generate-code arch=compute_75,code=[compute_75,sm_75] -DENABLE_BF16 -I/content/gpt2_optim/src -I../llm.c /content/gpt2_optim/src/profile_kvcache_optimization.cu -lcublas -lcublasLt -lnvidia-ml -lnvToolsExt  -o /content/gpt2_optim/bin/profile_kvcache_optimization\n",
            "/content/gpt2_optim/src/profile_kvcache_optimization.cu(46): warning #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "      multi_gpu_config = multi_gpu_config_init(1, 0, 1, empty_str, empty_str, \"mpi\");\n",
            "                                                                              ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "/content/gpt2_optim/src/profile_kvcache_optimization.cu(46): warning #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "      multi_gpu_config = multi_gpu_config_init(1, 0, 1, empty_str, empty_str, \"mpi\");\n",
            "                                                                              ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cd gpt2_optim && make all GPU_COMPUTE_CAPABILITY=75 PRECISION=BF16 LLM_C_ROOT=../llm.c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with KV Cache Optimization\n",
        "Command line arguments:\n",
        "- `-e`: specify model path (example: `../llm.c/gpt2_124M_bf16.bin`)\n",
        "- `-tk`: specify tokenizer path (example: `../llm.c/gpt2_tokenizer.bin`)\n",
        "- `-g`: specify number of tokens to generate (example: `64`)\n",
        "- `-b`: specify batch size (example: `4`)\n",
        "- `-m`: specify sampling method (example: `0` = random sampling, `1` = greedy sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-GPU support is disabled. Using a single GPU.\n",
            "[System]\n",
            "Device 0: Tesla T4\n",
            "Loading GPT-2 model from ../llm.c/gpt2_124M_bf16.bin\n",
            " -> max_seq_len: 1024\n",
            " -> vocab_size: 50257\n",
            " -> padded_vocab_size: 50304\n",
            " -> num_layers: 12\n",
            " -> num_heads: 12\n",
            " -> channels: 768\n",
            "allocating 2475 MiB for activations\n",
            "device memory usage: 2854 MiB / 14912 MiB\n",
            "memory per sequence: 618 MiB\n",
            " -> estimated maximum batch size: 23\n",
            "=== GPT-2 Inference (gpt2_optim) ===\n",
            "[Run Config]\n",
            "  checkpoint: ../llm.c/gpt2_124M_bf16.bin\n",
            "  tokenizer:  ../llm.c/gpt2_tokenizer.bin\n",
            "  genT:       64\n",
            "  batch size: 4\n",
            "  sampling:   random\n",
            "  validation: off\n",
            "===================================\n",
            "\n",
            "=== Section: Naive Inference (Baseline) ===\n",
            "\n",
            "Base implementation: total time = 5845.26 ms, forward time = 5723.97 ms\n",
            "Token per second: 43.80\n",
            "Generated tokens:\n",
            "Batch 0:\n",
            "!\n",
            "<|endoftext|>There were<|endoftext|>Forum Jump<|endoftext|>Copyright by WZZ<|endoftext|>The slow-motion shot gives you a glimpse at a picture that<|endoftext|>In case you<|endoftext|>Chennai: the government has<|endoftext|>Cover camera<|endoftext|>Scientists<|endoftext|>Republican presidential nominee<|endoftext|>sick profession: Luffy requires discipline in a<|endoftext|>Dimensions Twelve box\n",
            "===\n",
            "Batch 1:\n",
            "Lyndoguchi @alicewiedek has been<|endoftext|>Intro\n",
            "<|endoftext|>Gregory Lyon is a journalist and the<|endoftext|>El Kuno Timo communion: A luxury<|endoftext|>#Halo World Championships 2018 Progear has<|endoftext|>51 brave couples<|endoftext|>So next year EVE Online<|endoftext|>Under a court order this seats may\n",
            "===\n",
            "Batch 2:\n",
            "ws-getinfo.\" as \"csbath_GetInformationForChronoSpec_Analysis_4\", \"qOSGetInformationForCh<|endoftext|>LOS ANGEL<|endoftext|>Looking for a good budget camera when planning your trips?<|endoftext|>Here at Flighty, we<|endoftext|>Preview | Recap | Notebook<|endoftext|>GP Styles R\n",
            "===\n",
            "Batch 3:\n",
            "alstok Mtadokalstok gekskelsgekgek liillige klergie. Meals<|endoftext|>The Dream of Unglued Making A Mote en Flechacharger promises the historian and lover of olfactory history pleasures that 1990's the rediscovery will<|endoftext|>See\n",
            "===\n",
            "\n",
            "=== Section: KV Cache Inference ===\n",
            "\n",
            "KV Cache implementation: total time = 777.34 ms, forward time = 584.51 ms\n",
            "Token per second: 329.33\n",
            "Generated tokens:\n",
            "Batch 0:\n",
            "!\n",
            "<|endoftext|>Asmodeus Otecoâ€”White Mountain Time Travel 9.<|endoftext|>The Indianapolis Colts<|endoftext|>Pro-Amazon Web Services app with Bakery titles<|endoftext|>Letter to Fierce Muslim<|endoftext|>The Girl Who<|endoftext|>Social Club candidacy<|endoftext|>Justice League<|endoftext|>RTG also announced the tentative<|endoftext|>Advanced Thor-Revenge<|endoftext|>PETAL\n",
            "===\n",
            "Batch 1:\n",
            "Lyndog laughs as he<|endoftext|>Appropriate eating can help<|endoftext|>The parts of bird ear impressions<|endoftext|>7 pounds of all star baseball<|endoftext|>It was<|endoftext|>LISTEN: >> |<|endoftext|>The BBC called experts<|endoftext|>Senators<|endoftext|>Regardless<|endoftext|>SF Panthivka\n",
            "This summer something<|endoftext|>(CEBU union tentative<|endoftext|>\n",
            "===\n",
            "Batch 2:\n",
            "wu/ctionchat is up.\n",
            "\n",
            "<|endoftext|>Public places are<|endoftext|>Composed of about<|endoftext|>This lesson<|endoftext|>A mixture<|endoftext|>A Queer Nokia data centre<|endoftext|>Ivory issues 'play'<|endoftext|>Story highlights Mateen has<|endoftext|>Yahoo says Britain will<|endoftext|>What better way could someone<|endoftext|>Marko Valeria takes runners on\n",
            "===\n",
            "Batch 3:\n",
            "alot for gratitude for his success; he<|endoftext|>Over the course<|endoftext|>The board of committees<|endoftext|>Operation Home and Away at the Children's Hospital of Philadelphia in Baltimore, MD has<|endoftext|>\"MPDIPSI Click On Meet the<|endoftext|>New Hope Welling well service<|endoftext|>CLAIM\n",
            "<|endoftext|>Calcified Synthesis Recipes\n",
            "\n",
            "===\n",
            "\n",
            "Using random sampling, outputs will differ due to different numerical behavior.\n",
            "\n",
            "\n",
            "Comparing base and KV cache implementations...\n",
            "\n",
            " Batch 0:\n",
            "Mismatch at t=3: base=1858 kv=1722\n",
            "Mismatch at t=4: base=547 kv=14171\n",
            "Mismatch at t=5: base=50256 kv=385\n",
            "Mismatch at t=6: base=1890 kv=440\n",
            "Mismatch at t=7: base=388 kv=660\n",
            "Mismatch at t=8: base=15903 kv=1073\n",
            "Mismatch at t=9: base=50256 kv=960\n",
            "Mismatch at t=10: base=15269 kv=12256\n",
            "Mismatch at t=11: base=416 kv=9638\n",
            "Mismatch at t=12: base=370 kv=3862\n",
            "Mismatch at t=13: base=30148 kv=13524\n",
            "Mismatch at t=14: base=50256 kv=860\n",
            "Mismatch at t=15: base=464 kv=13\n",
            "Mismatch at t=16: base=3105 kv=50256\n",
            "Mismatch at t=17: base=12 kv=464\n",
            "Mismatch at t=18: base=38714 kv=19362\n",
            "Mismatch at t=19: base=2823 kv=20464\n",
            "Mismatch at t=20: base=3607 kv=50256\n",
            "Mismatch at t=21: base=345 kv=2964\n",
            "Mismatch at t=22: base=257 kv=12\n",
            "\n",
            " Batch 1:\n",
            "Mismatch at t=4: base=22200 kv=22051\n",
            "\n",
            " Batch 2:\n",
            "Mismatch at t=1: base=82 kv=84\n",
            "\n",
            " Batch 3:\n",
            "Mismatch at t=2: base=301 kv=313\n",
            "Token mismatches: 23 (showing up to 20)\n"
          ]
        }
      ],
      "source": [
        "# random sampling according to the logits distribution\n",
        "!cd gpt2_optim && ./bin/inference_gpt2optimcu \\\n",
        "  -e ../llm.c/gpt2_124M_bf16.bin \\\n",
        "  -tk ../llm.c/gpt2_tokenizer.bin \\\n",
        "  -g 64 -b 4 -m 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-GPU support is disabled. Using a single GPU.\n",
            "[System]\n",
            "Device 0: Tesla T4\n",
            "Loading GPT-2 model from ../llm.c/gpt2_124M_bf16.bin\n",
            " -> max_seq_len: 1024\n",
            " -> vocab_size: 50257\n",
            " -> padded_vocab_size: 50304\n",
            " -> num_layers: 12\n",
            " -> num_heads: 12\n",
            " -> channels: 768\n",
            "allocating 2475 MiB for activations\n",
            "device memory usage: 2854 MiB / 14912 MiB\n",
            "memory per sequence: 618 MiB\n",
            " -> estimated maximum batch size: 23\n",
            "=== GPT-2 Inference (gpt2_optim) ===\n",
            "[Run Config]\n",
            "  checkpoint: ../llm.c/gpt2_124M_bf16.bin\n",
            "  tokenizer:  ../llm.c/gpt2_tokenizer.bin\n",
            "  genT:       64\n",
            "  batch size: 4\n",
            "  sampling:   argmax\n",
            "  validation: off\n",
            "===================================\n",
            "\n",
            "=== Section: Naive Inference (Baseline) ===\n",
            "\n",
            "Base implementation: total time = 5845.68 ms, forward time = 5821.09 ms\n",
            "Token per second: 43.79\n",
            "Generated tokens:\n",
            "Batch 0:\n",
            "!\n",
            "\n",
            "The first thing I did was to go to the local store and buy a few of the \"B\"s. I was told that the Bs were the best I had ever had. I was so excited to try them out. I was so excited to try them out. I was so excited to\n",
            "===\n",
            "Batch 1:\n",
            "L.A.\n",
            "\n",
            "The first of the two-part series, \"The New York Times,\" will be published in the fall.\n",
            "\n",
            "The first of the two-part series, \"The New York Times,\" will be published in the fall.\n",
            "\n",
            "The first of the two-part series, \"\n",
            "===\n",
            "Batch 2:\n",
            "w.com/news/local/local-news/local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-\n",
            "===\n",
            "Batch 3:\n",
            ".\n",
            "\n",
            "The first of the two, the \"B\" in the name of the \"B\" in the name of the \"B\" in the name of the \"B\" in the name of the \"B\" in the name of the \"B\" in the name of the \"B\" in the\n",
            "===\n",
            "\n",
            "=== Section: KV Cache Inference ===\n",
            "\n",
            "KV Cache implementation: total time = 607.33 ms, forward time = 581.89 ms\n",
            "Token per second: 421.51\n",
            "Generated tokens:\n",
            "Batch 0:\n",
            "!\n",
            "\n",
            "The first thing I noticed was that the \"C\" in the name of the \"C\" was a reference to the \"C\" in the name of the \"C\" in the \"C\" in the \"C\" in the \"C\" in the \"C\" in the \"C\" in\n",
            "===\n",
            "Batch 1:\n",
            "L.A.\n",
            "\n",
            "The first of the two new teams to enter the league, the New York Red Bulls, will be the first team to play in the MLS Cup Playoffs.\n",
            "\n",
            "The Red Bulls will play their first match of the season on Sunday, May 1 at Toyota Park in Houston.\n",
            "\n",
            "The\n",
            "===\n",
            "Batch 2:\n",
            "w.com/news/local/local-news/local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-local-news-\n",
            "===\n",
            "Batch 3:\n",
            ".\n",
            "\n",
            "The first of the two, the \"B\" in the name of the \"B\" in the name of the \"B\" in the name of the \"B\" in the name of the \"B\" in the name of the \"B\" in the name of the \"B\" in the\n",
            "===\n",
            "\n",
            "Using argmax sampling, outputs should match almost identically.\n",
            "\n",
            "\n",
            "Comparing base and KV cache implementations...\n",
            "\n",
            " Batch 0:\n",
            "Mismatch at t=7: base=750 kv=6810\n",
            "Mismatch at t=9: base=284 kv=326\n",
            "Mismatch at t=10: base=467 kv=262\n",
            "Mismatch at t=11: base=284 kv=366\n",
            "Mismatch at t=12: base=262 kv=34\n",
            "Mismatch at t=13: base=1957 kv=1\n",
            "Mismatch at t=14: base=3650 kv=287\n",
            "Mismatch at t=15: base=290 kv=262\n",
            "Mismatch at t=16: base=2822 kv=1438\n",
            "Mismatch at t=17: base=257 kv=286\n",
            "Mismatch at t=18: base=1178 kv=262\n",
            "Mismatch at t=19: base=286 kv=366\n",
            "Mismatch at t=20: base=262 kv=34\n",
            "Mismatch at t=21: base=366 kv=1\n",
            "Mismatch at t=22: base=33 kv=373\n",
            "Mismatch at t=23: base=1 kv=257\n",
            "Mismatch at t=24: base=82 kv=4941\n",
            "Mismatch at t=25: base=13 kv=284\n",
            "Mismatch at t=26: base=314 kv=262\n",
            "Mismatch at t=27: base=373 kv=366\n",
            "\n",
            " Batch 1:\n",
            "Mismatch at t=11: base=12 kv=649\n",
            "\n",
            " Batch 2:\n",
            "\n",
            " Batch 3:\n",
            "Token mismatches: 21 (showing up to 20)\n"
          ]
        }
      ],
      "source": [
        "# Greedy sampling (always pick the token with the highest logit)\n",
        "!cd gpt2_optim && ./bin/inference_gpt2optimcu \\\n",
        "  -e ../llm.c/gpt2_124M_bf16.bin \\\n",
        "  -tk ../llm.c/gpt2_tokenizer.bin \\\n",
        "  -g 64 -b 4 -m 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-GPU support is disabled. Using a single GPU.\n",
            "[System]\n",
            "Device 0: Tesla T4\n",
            "Loading GPT-2 model from ../llm.c/gpt2_124M_bf16.bin\n",
            " -> max_seq_len: 1024\n",
            " -> vocab_size: 50257\n",
            " -> padded_vocab_size: 50304\n",
            " -> num_layers: 12\n",
            " -> num_heads: 12\n",
            " -> channels: 768\n",
            "=== KV Cache Validation ===\n",
            "[Run Config]\n",
            "  checkpoint: ../llm.c/gpt2_124M_bf16.bin\n",
            "  tokenizer:  ../llm.c/gpt2_tokenizer.bin\n",
            "  genT:       32\n",
            "  batch size: 2\n",
            "  precision:  BF16\n",
            "===========================\n",
            "\n",
            "allocating 1237 MiB for activations\n",
            "device memory usage: 1616 MiB / 14912 MiB\n",
            "memory per sequence: 618 MiB\n",
            " -> estimated maximum batch size: 23\n",
            "Step | max_abs_diff | rmse      | base_top3(token:val)                 | opt_top3(token:val)\n",
            "-----+--------------+-----------+-------------------------------------+-------------------------------------\n",
            "   0 |     0.000000 |  0.000000 |    11: -28.3750    13: -28.5000    11: -28.7500 |    11: -28.3750    13: -28.5000    11: -28.7500\n",
            "   1 |     0.500000 |  0.267544 |   290: -78.5000   262: -79.0000   543: -79.0000 |   290: -78.5000   262: -79.0000   475: -79.0000\n",
            "   2 |     2.000000 |  0.811436 |   257: -67.5000   262: -67.5000   407: -68.0000 |   257: -68.0000   262: -68.0000   407: -68.5000\n",
            "   3 |     0.500000 |  0.294186 |   717: -70.5000   366: -71.0000   649: -71.0000 |   717: -70.5000   366: -71.0000   649: -71.0000\n",
            "   4 |     0.500000 |  0.306442 |   198: -61.0000   383: -62.7500   366: -63.2500 |   198: -61.2500   383: -63.0000   366: -63.2500\n",
            "   5 |     0.500000 |  0.292042 |    13: -55.5000     8: -55.7500    11: -57.7500 |    13: -55.5000     8: -55.7500    25: -57.5000\n",
            "   6 |     1.000000 |  0.427625 |    83: -59.2500   796: -59.5000   489: -59.7500 |    83: -59.5000   796: -59.7500   489: -60.0000\n",
            "   7 |     1.000000 |  0.563166 |    64: -62.7500    87: -63.0000    88: -63.2500 |    64: -62.0000    87: -62.5000    88: -62.7500\n",
            "   8 |     2.000000 |  1.170316 |   262: -45.5000   198: -47.0000   290: -47.7500 |   262: -47.0000   198: -48.5000   290: -49.2500\n",
            "   9 |     1.000000 |  0.475509 |   262: -53.5000   198: -54.0000  1169: -54.7500 |   262: -54.0000   198: -54.5000  1169: -55.2500\n",
            "  10 |     0.750000 |  0.350806 |    25: -39.5000    11: -40.0000    13: -41.0000 |    25: -40.0000    11: -40.5000    13: -41.5000\n",
            "  11 |     0.750000 |  0.317855 |    11: -37.7500    25: -38.0000    13: -38.7500 |    11: -38.0000    25: -38.2500    13: -39.0000\n",
            "  12 |     1.000000 |  0.388005 |    82: -53.5000    25: -54.2500   259: -54.2500 |    82: -53.7500    25: -54.5000    83: -54.5000\n",
            "  13 |     0.500000 |  0.282395 |    12: -41.7500    25: -42.2500    11: -42.7500 |    12: -41.5000    25: -41.7500    11: -42.5000\n",
            "  14 |     0.500000 |  0.276861 |   262: -60.5000   307: -61.5000    25: -61.7500 |   262: -60.2500   307: -61.2500    25: -61.5000\n",
            "  15 |     1.000000 |  0.381628 |  1350: -50.0000  4598: -50.2500  1169: -50.5000 |  1350: -49.7500  4598: -50.0000  1169: -50.2500\n",
            "  16 |     0.500000 |  0.372966 |    13: -56.2500    11: -57.2500    25: -57.5000 |    13: -55.7500    11: -57.0000    25: -57.0000\n",
            "  17 |     0.500000 |  0.345200 |   198: -55.5000   532: -55.5000   383: -56.0000 |   532: -55.2500   198: -55.5000   383: -55.7500\n",
            "  18 |     0.500000 |  0.202431 |   532: -54.0000   198: -54.2500   383: -54.7500 |   198: -54.0000   532: -54.0000   383: -54.7500\n",
            "  19 |     1.000000 |  0.557789 |   532: -43.2500    13: -43.5000   198: -43.7500 |   532: -44.0000    13: -44.2500   198: -44.5000\n",
            "  20 |     1.000000 |  0.579537 |    13:  -2.8281   532:  -4.6875   764:  -4.9062 |    13:  -2.5469   532:  -4.5000   764:  -4.6250\n",
            "  21 |     1.375000 |  0.836535 |    13:  -4.5000    12:  -6.6875   532:  -7.2812 |    13:  -3.3594    12:  -5.4062   532:  -6.1250\n",
            "  22 |     0.500000 |  0.348297 |    13: -60.0000    11: -60.7500    12: -61.0000 |    13: -59.7500    11: -60.2500    12: -60.7500\n",
            "  23 |     0.500000 |  0.223424 |    12: -31.7500    13: -31.7500   532: -31.7500 |    12: -31.6250    13: -31.6250   532: -31.6250\n",
            "  24 |     1.000000 |  0.460028 |    12: -34.0000    13: -34.0000   532: -34.2500 |    12: -33.5000    13: -33.5000   532: -33.7500\n",
            "  25 |     0.500000 |  0.218787 |    13: -30.8750    11: -31.3750    12: -31.6250 |    13: -31.1250    11: -31.6250    12: -31.8750\n",
            "  26 |     1.250000 |  0.651273 |    12: -50.0000    13: -50.0000    11: -50.5000 |    12: -50.7500    13: -50.7500    11: -51.5000\n",
            "  27 |     0.500000 |  0.212010 |    13: -46.7500    82: -47.0000    11: -48.0000 |    13: -46.7500    82: -47.0000    11: -47.7500\n",
            "  28 |     0.500000 |  0.248683 |    13: -44.7500    12: -45.2500    11: -45.5000 |    13: -44.5000    12: -45.2500    11: -45.5000\n",
            "  29 |     1.000000 |  0.373615 |    12: -43.7500   198: -44.2500    64: -45.5000 |    12: -44.0000   198: -44.5000    64: -45.7500\n",
            "  30 |     0.750000 |  0.362043 |    13: -48.5000    50: -48.5000    55: -48.5000 |    50: -48.0000    55: -48.0000    13: -48.2500\n",
            "  31 |     0.750000 |  0.342145 |   532: -42.5000   198: -43.5000 15917: -44.7500 |   532: -42.2500   198: -43.2500 15917: -44.5000\n"
          ]
        }
      ],
      "source": [
        "!cd gpt2_optim && ./bin/validate_kvcache_optimization \\\n",
        "  -e ../llm.c/gpt2_124M_bf16.bin \\\n",
        "  -tk ../llm.c/gpt2_tokenizer.bin \\\n",
        "  -g 32 -b 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Nsight Systems (nsys), might take a few minutes\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2025.5.2_2025.5.2.266-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2025.5.2_2025.5.2.266-1_amd64.deb\n",
        "!apt --fix-broken install\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-GPU support is disabled. Using a single GPU.\n",
            "[System]\n",
            "Device 0: Tesla T4\n",
            "Loading GPT-2 model from ../llm.c/gpt2_124M_bf16.bin\n",
            " -> max_seq_len: 1024\n",
            " -> vocab_size: 50257\n",
            " -> padded_vocab_size: 50304\n",
            " -> num_layers: 12\n",
            " -> num_heads: 12\n",
            " -> channels: 768\n",
            "allocating 1237 MiB for activations\n",
            "device memory usage: 1616 MiB / 14912 MiB\n",
            "memory per sequence: 618 MiB\n",
            " -> estimated maximum batch size: 23\n"
          ]
        }
      ],
      "source": [
        "!cd gpt2_optim && ./bin/profile_kvcache_optimization \\\n",
        "    -e ../llm.c/gpt2_124M_bf16.bin \\\n",
        "    -tk ../llm.c/gpt2_tokenizer.bin \\\n",
        "    -g 128 -b 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting data...\n",
            "Multi-GPU support is disabled. Using a single GPU.\n",
            "[System]\n",
            "Device 0: Tesla T4\n",
            "Loading GPT-2 model from ../llm.c/gpt2_124M_bf16.bin\n",
            " -> max_seq_len: 1024\n",
            " -> vocab_size: 50257\n",
            " -> padded_vocab_size: 50304\n",
            " -> num_layers: 12\n",
            " -> num_heads: 12\n",
            " -> channels: 768\n",
            "allocating 1237 MiB for activations\n",
            "device memory usage: 1644 MiB / 14912 MiB\n",
            "memory per sequence: 618 MiB\n",
            " -> estimated maximum batch size: 23\n",
            "Generating '/tmp/nsys-report-93e9.qdstrm'\n",
            "[1/1] [========================100%] prof_kvcache.nsys-rep\n",
            "Generated:\n",
            "\t/content/gpt2_optim/prof_kvcache.nsys-rep\n"
          ]
        }
      ],
      "source": [
        "!cd gpt2_optim && nsys profile -t cuda,nvtx \\\n",
        "  -o prof_kvcache \\\n",
        "  ./bin/profile_kvcache_optimization \\\n",
        "    -e ../llm.c/gpt2_124M_bf16.bin \\\n",
        "    -tk ../llm.c/gpt2_tokenizer.bin \\\n",
        "    -g 128 -b 2\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
